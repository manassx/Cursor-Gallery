# AI Integration Context for Vibestate Hackathon

## Project Overview

**Vibestate** is a creative portfolio platform combining a web app and Android companion app. The
unique feature is a cursor-based image trail where images appear as you move the cursor/finger
across the canvas. The goal is to win the hackathon by integrating **RunAnywhere SDK** (on-device
AI) in the most creative and impactful way possible.

## Current Tech Stack

### Android App (`D:/projects/CursorGallery/vibestate/android app`)

- **Language**: Kotlin + Jetpack Compose
- **Architecture**: MVVM with ViewModels, Coroutines, StateFlow
- **Backend**: Supabase (via Flask API proxy)
- **Auth**: Google Sign-In
- **Navigation**: Compose Navigation with routes for Splash, Auth, Dashboard, Create, Editor,
  Viewer, Settings
- **Theme**: Brutalist design with dark/light modes, bold sans-serif headings, serif body copy,
  grain textures, border accents
- **Key Screens**:
    - `DashboardScreen`: Shows portfolios, quick actions (Create, Edit, Your Collection, **AI Studio
      **)
    - `CreatePortfolioScreen`: Upload images, set title/description
    - `GalleryEditorScreen`: Edit threshold, transform images, adjust canvas, **AI Tools button**
    - `GalleryViewerScreen`: View the final cursor trail experience

### Web App

- **Frontend** (`D:/projects/CursorGallery/vibestate/frontend/`): React + Vite + Tailwind, Zustand
  state management
- **Backend** (`D:/projects/CursorGallery/vibestate/backend/`): Flask + Supabase
- **Features**: Dashboard, gallery editor, viewer, settings
- **Design**: Same brutalist aesthetic as Android app

## RunAnywhere SDK Integration (Current State)

### âœ… COMPLETED

#### 1. Core Infrastructure

**Location**: `app/src/main/java/com/cursorgallery/ai/`

- **`AiConfig.kt`**: Feature toggle (`AI_FEATURES_ENABLED = true`), model definitions (Qwen 2.5 0.5B
  Instruct, SmolLM2 360M, Llama 3.2 1B)
- **`RunAnywhereManager.kt`**: Singleton manager wrapping SDK initialization, model lifecycle (
  download, load, unload), state tracking via StateFlow
- **`AiOrchestrator.kt`**: High-level orchestration layer for running prompts, streaming responses,
  parsing JSON outputs (mood presets, sequence plans, critiques)
- **Initialization**: Wired into **`com.runanywhere.startup_hackathon20.CursorGalleryApp`** (NOT
  `com.cursorgallery.CursorGalleryApp` - there are two Application classes!)

#### 2. ViewModel Layer

**Location**: `app/src/main/java/com/cursorgallery/viewmodel/`

- **`AiStudioViewModel.kt`**:
    - Manages AI Studio UI state (model list, download progress, active model)
    - Exposes actions: `runMoodPreset()`, `runSequencePlan()`, `runCritique()`
    - Tracks processing state, results, and errors
- **`GalleryEditorViewModel.kt`**:
    - Updated with methods to apply AI outputs: `updateGalleryConfig()`, `applyImageOrdering()`

#### 3. UI Components

**AI Studio Screen** (`app/src/main/java/com/cursorgallery/ui/screens/ai/AiStudioBlueprint.kt`):

- Lists available models with download/load/unload controls
- Shows SDK status card (Ready/Not initialized/Failed)
- Displays 3 models: Qwen 2.5 0.5B, SmolLM2 360M, Llama 3.2 1B
- Download buttons show progress, turn into "Load Model" when complete
- Accessible from Dashboard via "AI Studio" card

**Gallery Editor Integration** (
`app/src/main/java/com/cursorgallery/ui/screens/portfolio/GalleryEditorScreen.kt`):

- **AI Tools button** in top-right toolbar (next to VIEW button)
- Opens a **ModalBottomSheet** with `AiCompanionPanel` containing:
    - Model status indicator
    - Atmosphere prompt text field
    - **"Compose Atmosphere"** button â†’ generates mood presets (animation type, mood, color palette)
    - **"Sequence Oracle"** button â†’ analyzes images and suggests optimal viewing order
    - **"Critique Portfolio"** button â†’ scores composition/emotion/storytelling, provides
      highlights/recommendations
    - Result cards display AI outputs with "Apply" buttons
    - Loading indicators and error messages

**Dashboard Enhancement**:

- Large "AI Studio" card on left (hero position)
- Creative copy: "Palette Architect, Sequence Oracle, Offline Critic" instead of generic "mood"
  language
- Edit Portfolio and Your Collection cards stacked compactly on right

#### 4. Data Models

**Location**: `app/src/main/java/com/cursorgallery/ai/AiActionBlueprints.kt`

- `MoodPresetUiModel`: title, description, animationType, mood, colorPalette
- `SequencePlanUiModel`: ordered list of image IDs with rationales
- `CritiqueReportUiModel`: scores (overall, composition, emotion, storytelling), highlights,
  recommendations

#### 5. Model Strategy

- **Primary**: **Qwen 2.5 0.5B Instruct Q6_K** (~380 MB, strong reasoning, best quality)
- **Fallback**: **SmolLM2 360M Q8_0** (lightweight, for quick drafts or low-memory devices)
- **Premium**: **Llama 3.2 1B Instruct Q6_K** (optional, for demo "wow factor")

## ðŸš¨ CRITICAL DEBUGGING INFO (Must Read!)

### Application Class Confusion (FIXED âœ…)

**Problem**: There are TWO Application classes in the codebase:

1. `com.cursorgallery.CursorGalleryApp` (created for AI, NOT actually running)
2. `com.runanywhere.startup_hackathon20.CursorGalleryApp` (the one AndroidManifest points to)

**Solution**: We added RunAnywhere initialization to the CORRECT Application class at
`app/src/main/java/com/runanywhere/startup_hackathon20/CursorGalleryApp.kt`:

```kotlin
class CursorGalleryApp : Application() {
    override fun onCreate() {
        super.onCreate()
        // Initialize RunAnywhere SDK
        RunAnywhereManager.initialize(this)
    }
}
```

**Key Points**:

- MUST use `GlobalScope.launch(Dispatchers.IO)` for SDK initialization (per official docs)
- Call `LlamaCppServiceProvider.register()` during init
- Call `scanForDownloadedModels()` to find previously downloaded models
- Check `RunAnywhereManager.state` to verify initialization completed

### SDK Initialization Checklist

âœ… **What Works Now**:

- SDK initializes successfully on app startup
- AI Studio shows "SDK Status: Ready" with green checkmark
- 3 models appear in the list (Qwen, SmolLM2, Llama)
- Download buttons work, show progress
- Models download successfully (~380MB for Qwen)
- Load Model button works, model loads into memory
- Card turns green when model loaded

### Model Inference Issue (CURRENT BLOCKER âŒ)

**Problem**: Model loads successfully but `RunAnywhere.generateStream()` returns 0 tokens.

**Symptoms**:

```
D AiOrchestrator: Calling RunAnywhere.generateStream with prompt length: XXX
D AiOrchestrator: Received full response length: 0
D AiOrchestrator: Raw response: 
W System.err: Sanitized JSON: 
E AiOrchestrator: JSON parsing returned null. Raw response:
```

**What We Tried**:

1. âœ… Fixed Application class (was the main issue)
2. âœ… Changed from `CoroutineScope` to `GlobalScope` (per docs)
3. âœ… Used `generateStream()` with `.fold()` (matching ChatViewModel example)
4. âœ… Simplified prompts (removed Qwen chat template)
5. âŒ Still generates 0 tokens

**Current Code Structure**:

```kotlin
// AiOrchestrator.kt
suspend fun executeJsonRequest(prompt: String, parse: (String) -> T): Result<T> {
    var fullResponse = ""
    RunAnywhere.generateStream(prompt).fold(fullResponse) { acc, token ->
        acc + token
    }
    // fullResponse.length == 0 (ISSUE!)
}
```

**Logs Show**:

- Model loads: `Model -1841487817 loaded successfully into LLM service`
- But generation produces no tokens
- No crash, no error - just empty output

**Possible Causes** (for next agent to investigate):

1. Generation parameters missing (max_tokens, temperature) - SDK doesn't expose these in current API
2. Prompt format issue - model might need special tokens/format
3. Model file corruption - might need re-download
4. SDK bug - inference engine not connecting properly
5. EOS token fired immediately - model thinks prompt is complete

**Important Files**:

- `app/src/main/java/com/cursorgallery/ai/AiOrchestrator.kt` (generation logic)
- `app/src/main/java/com/cursorgallery/ai/AiBlueprintDocumentation.kt` (prompts)
- `app/src/main/java/com/cursorgallery/ai/RunAnywhereManager.kt` (SDK wrapper)
- `app/src/main/java/com/runanywhere/startup_hackathon20/ChatViewModel.kt` (working example from
  SDK)

**Working Example in Codebase**: `ChatViewModel.kt` uses the EXACT same API (
`RunAnywhere.generateStream(text).collect { token -> ... }`), so the SDK CAN work - we're missing
something.

## Next Steps (Priority Order for Next Agent)

### ðŸ”¥ URGENT: Fix Model Inference (2-3 hours)

**The blocker**: Model loads but generates 0 tokens. THIS MUST BE FIXED FIRST.

**Debug Strategy**:

1. **Compare with working example**:
    - Look at `app/src/main/java/com/runanywhere/startup_hackathon20/ChatViewModel.kt`
    - It uses identical API and works in the original hackathon starter
    - What's different? (initialization order? model selection? prompt format?)

2. **Test with ChatViewModel directly**:
    - Try using the existing ChatViewModel in AI Studio
    - If IT generates text, then our prompt/orchestrator has issues
    - If it ALSO generates 0 tokens, then SDK/model is the problem

3. **Check SDK logs in detail**:
   ```powershell
   adb logcat -d | Select-String "RunAnywhere|LlamaCpp|Generation"
   ```
    - Look for generation parameters being set
    - Check if model is actually in "ready" state
    - Verify no silent errors

4. **Try different model**:
    - Download SmolLM2 360M instead
    - Load it and test generation
    - If it works â†’ Qwen model issue
    - If it fails too â†’ broader SDK problem

5. **Test ultra-simple prompt**:
   ```kotlin
   RunAnywhere.generateStream("Hello").collect { token ->
       Log.d("TEST", "Token: $token")
   }
   ```
    - If this generates nothing, SDK is broken
    - If this works, our prompts are the issue

6. **Check generation configuration** (if SDK exposes it):
    - Look for `GenerationConfig` or similar classes
    - Try setting `maxTokens = 100`, `temperature = 0.7`
    - SDK documentation mentions these parameters exist

7. **Contact RunAnywhere support** (if stuck after 2 hours):
    - This is a hackathon - they WANT you to succeed
    - Share logs, model ID, initialization code
    - Ask if there's a known issue with Qwen 2.5 0.5B

**Success Criteria**: `RunAnywhere.generateStream()` returns at least 1 token for ANY prompt.

### Phase 1: Get Models Working (after fixing inference)

1. **Test model download flow**: Load Qwen 2.5 0.5B in AI Studio, verify no crashes
2. **Wire actual inference**: Make "Compose Atmosphere" button call real model, parse response
3. **Debug prompts**: Ensure JSON outputs match expected structure
4. **Apply presets**: Hook mood presets to actual canvas config changes

### Phase 2: Complete Core Features (4-5 hours)

5. **Image metadata extraction**: For sequencing, analyze colors/brightness per image
6. **Sequence validation**: Test oracle with real portfolios, refine rationale prompts
7. **Critique enhancement**: Improve scoring logic, add more insightful recommendations
8. **Workflow automations**: Add "Write description" button in CreatePortfolioScreen

### Phase 3: Backend Sync (2-3 hours)

9. **API endpoints**: POST AI results to Flask (`/api/galleries/{id}/ai-insights`)
10. **Supabase schema**: Add `ai_insights` JSONB column to galleries table
11. **Web display**: Show critique/mood/sequence on gallery detail page

### Phase 4: Polish & Demo (2-3 hours)

12. **UI animations**: Smooth transitions, loading states, success feedback
13. **Error handling**: Graceful degradation if model fails
14. **Demo portfolio**: Create a showcase portfolio with before/after examples
15. **Walkthrough script**: Document the demo flow for judges

## Technical Architecture

### Key Integration Points

```
CursorGalleryApp
  â””â”€ RunAnywhereManager.initialize()

DashboardScreen
  â”œâ”€ AI Studio Card â†’ AiStudioScreen
  â””â”€ Quick Actions

GalleryEditorScreen
  â”œâ”€ AI Tools Button â†’ ModalBottomSheet
  â”‚   â””â”€ AiCompanionPanel
  â”‚       â”œâ”€ AiStudioViewModel (AI actions)
  â”‚       â””â”€ GalleryEditorViewModel (apply results)
  â””â”€ Canvas (TouchTrailCanvas)

AiStudioScreen
  â””â”€ Model Management UI
      â”œâ”€ Download models
      â”œâ”€ Load/Unload
      â””â”€ Show planned actions
```

### Data Flow

1. User taps "AI Tools" in editor
2. Bottom sheet opens, shows model status
3. User enters prompt / taps action button
4. `AiStudioViewModel` calls `AiOrchestrator`
5. Orchestrator runs prompt via `RunAnywhereManager`
6. Model streams response (JSON)
7. Orchestrator parses JSON â†’ UI model
8. ViewModel updates state â†’ UI shows result card
9. User taps "Apply" â†’ `GalleryEditorViewModel` updates gallery
10. Gallery patches to backend â†’ syncs to web

### Reversibility (Critical for Hackathon Safety)

- **Feature toggle**: Set `AiConfig.AI_FEATURES_ENABLED = false` to disable all AI instantly
- **Isolated package**: All AI code in `com.cursorgallery.ai/`, easy to remove
- **No modifications to core flows**: Existing screens work unchanged without AI
- **Optional UI**: AI Tools button/panel only appears when feature is enabled

## Current Progress: ~60% Complete

### What Works Right Now

âœ… SDK initialized and models registered  
âœ… AI Studio screen accessible from Dashboard  
âœ… AI Tools button in Gallery Editor  
âœ… Bottom sheet panel with all three actions  
âœ… Mood preset generation and display  
âœ… Sequence ordering suggestion and application  
âœ… Portfolio critique scoring and display  
âœ… Loading states and error handling  
âœ… Theme-consistent UI matching brutalist design  
âœ… Google auto-login (no re-sign-in every launch)

### What's Missing for Hackathon Win

ðŸ”² **Model download UI polish**: Show progress, handle errors gracefully  
ðŸ”² **Actual model inference**: Currently buttons exist but no model is loaded/tested  
ðŸ”² **Enhanced prompts**: Current prompts are placeholders, need creative tuning  
ðŸ”² **Image metadata extraction**: For sequencing, need to analyze actual image properties  
ðŸ”² **Backend sync**: AI outputs need to POST to Flask API â†’ Supabase  
ðŸ”² **Web dashboard display**: Show AI insights (critique, mood, sequence) on website  
ðŸ”² **Workflow automations**: "Write description", "Generate social copy", etc.  
ðŸ”² **Viewer chat**: Contextual Q&A for visitors  
ðŸ”² **Visual polish**: Animations, transitions, before/after comparisons  
ðŸ”² **Demo script**: Prepared walkthrough highlighting AI features for judges

## Hackathon Strategy: How to Win

### 1. **Privacy-First Narrative**

- RunAnywhere = on-device, no data leaves phone
- Perfect for creative professionals worried about IP theft
- Offline-first means curators can work anywhere

### 2. **Creative Differentiation**

- Don't just add generic chatbots
- Focus on **creative tools**: atmosphere architect, sequence oracle, AI critic
- Language matters: "Palette Architect" not "Color Picker", "Sequence Oracle" not "Auto Sort"

### 3. **Concrete Productivity**

- Judges love measurable value: "Save 2 hours writing gallery descriptions"
- Show before/after: manual sequencing vs AI-sequenced trail

### 4. **Cross-Platform Sync**

- AI insights generated on mobile â†’ visible on web dashboard
- Unique selling point: mobile AI brain, web consumption

### 5. **Polish Over Features**

- Better to have 3 features that work beautifully than 10 half-broken ones
- Focus on: Mood DJ + Sequencing + Critique (core trio)

### 6. **Live Demo Magic**

- Prepare a portfolio with dramatic sequencing improvement
- Show critique scores live
- Generate atmosphere presets on stage

## Models to Use

### Primary: Qwen 2.5 0.5B Instruct Q6_K

- **Size**: ~380 MB
- **Strengths**: Strong instruction following, good creative writing, handles JSON well
- **Use for**: Mood presets, critiques, workflow automations
- **Download**: Via RunAnywhere ModelCatalog

### Fallback: SmolLM2 360M Q8_0

- **Size**: ~220 MB
- **Strengths**: Fast, tiny, good for quick drafts
- **Use for**: Low-memory devices, speed demonstrations
- **Download**: Via RunAnywhere ModelCatalog

### Premium (Optional): Llama 3.2 1B Q6_K

- **Size**: ~700 MB
- **Strengths**: Best quality, most coherent long-form text
- **Use for**: "High fidelity" mode toggle, demo wow factor
- **Download**: Via RunAnywhere ModelCatalog

## Prompt Templates (Ready to Use)

### Mood Preset

```kotlin
"""
Analyze this portfolio context and create an atmospheric preset.

Portfolio title: ${gallery.title}
Description: ${gallery.description}
Number of images: ${images.size}
User vibe request: "$userPrompt"

Generate a JSON response with this structure:
{
  "title": "Short evocative preset name",
  "description": "One sentence capturing the essence",
  "animationType": "float|fade|glide|pulse|orbit",
  "mood": "nostalgic|energetic|serene|dramatic|playful",
  "colorPalette": ["#hex1", "#hex2", "#hex3"]
}
"""
```

### Sequence Plan

```kotlin
"""
Analyze these images and suggest optimal viewing order for maximum visual impact.

Images:
${images.mapIndexed { idx, img -> "$idx. ${img.url} - ${img.description}" }.joinToString("\n")}

Consider: color harmony, emotional progression, composition flow.

Return JSON:
{
  "entries": [
    {"imageId": "uuid", "rationale": "Why this position"},
    ...
  ]
}
"""
```

### Portfolio Critique

```kotlin
"""
Review this creative portfolio and provide constructive feedback.

Title: ${gallery.title}
Description: ${gallery.description}
Images: ${images.size} pieces

Evaluate on:
- Composition (technical quality)
- Emotional resonance
- Storytelling coherence

Return JSON:
{
  "overallScore": 0-100,
  "compositionScore": 0-100,
  "emotionScore": 0-100,
  "storytellingScore": 0-100,
  "highlights": ["strength 1", "strength 2"],
  "recommendations": ["improvement 1", "improvement 2"]
}
"""
```

## Files Reference (For Next Agent)

### Core AI Files (Already Exist)

- `app/src/main/java/com/cursorgallery/ai/AiConfig.kt`
- `app/src/main/java/com/cursorgallery/ai/RunAnywhereManager.kt`
- `app/src/main/java/com/cursorgallery/ai/AiOrchestrator.kt`
- `app/src/main/java/com/cursorgallery/ai/AiActionBlueprints.kt`
- `app/src/main/java/com/cursorgallery/viewmodel/AiStudioViewModel.kt`
- `app/src/main/java/com/cursorgallery/ui/screens/ai/AiStudioBlueprint.kt`
- `app/src/main/java/com/cursorgallery/ui/screens/portfolio/GalleryEditorScreen.kt`

### Integration Points to Modify

- `app/src/main/java/com/cursorgallery/viewmodel/GalleryEditorViewModel.kt` (apply AI results)
- `app/src/main/java/com/cursorgallery/data/remote/ApiClient.kt` (sync AI to backend)
- `backend/app.py` (receive AI payloads)
- `frontend/src/pages/Dashboard.jsx` (display AI insights)

## Next Steps (Priority Order for Next Agent)

### Phase 1: Get Models Working (3-4 hours)

1. **Test model download flow**: Load Qwen 2.5 0.5B in AI Studio, verify no crashes
2. **Wire actual inference**: Make "Compose Atmosphere" button call real model, parse response
3. **Debug prompts**: Ensure JSON outputs match expected structure
4. **Apply presets**: Hook mood presets to actual canvas config changes

### Phase 2: Complete Core Features (4-5 hours)

5. **Image metadata extraction**: For sequencing, analyze colors/brightness per image
6. **Sequence validation**: Test oracle with real portfolios, refine rationale prompts
7. **Critique enhancement**: Improve scoring logic, add more insightful recommendations
8. **Workflow automations**: Add "Write description" button in CreatePortfolioScreen

### Phase 3: Backend Sync (2-3 hours)

9. **API endpoints**: POST AI results to Flask (`/api/galleries/{id}/ai-insights`)
10. **Supabase schema**: Add `ai_insights` JSONB column to galleries table
11. **Web display**: Show critique/mood/sequence on gallery detail page

### Phase 4: Polish & Demo (2-3 hours)

12. **UI animations**: Smooth transitions, loading states, success feedback
13. **Error handling**: Graceful degradation if model fails
14. **Demo portfolio**: Create a showcase portfolio with before/after examples
15. **Walkthrough script**: Document the demo flow for judges

## Testing Checklist

### Must Test Before Submission

- [ ] Download Qwen 2.5 0.5B without crashes
- [ ] Load model successfully (check RunAnywhereManager state)
- [ ] Generate mood preset with valid JSON
- [ ] Apply preset to gallery, see config change
- [ ] Run sequence oracle, verify reordering
- [ ] Run critique, see score card
- [ ] Apply sequence, confirm images reorder on canvas
- [ ] Sync results to backend, check Supabase
- [ ] View AI insights on web dashboard
- [ ] Test offline (airplane mode), confirm AI still works
- [ ] Low-memory fallback (try SmolLM2)
- [ ] Google auto-login works after app restart

## Common Issues & Solutions

### Model Won't Load

- Check storage permissions in AndroidManifest.xml
- Verify model file downloaded to correct directory
- Check RunAnywhere logs for initialization errors

### JSON Parsing Fails

- Wrap orchestrator calls in try-catch
- Add fallback to regex extraction if JSON malformed
- Log raw model output for debugging

### UI Freezes During Inference

- Ensure all model calls inside `viewModelScope.launch`
- Use `Dispatchers.Default` for CPU-bound work
- Show loading indicator immediately on button tap

### Backend Sync Issues

- Check TokenManager has valid JWT
- Verify Flask CORS allows POST from Android
- Log full API response for debugging

## Winning Pitch (For Judges)

> "Vibestate brings AI creativity tools directly to your fingertipsâ€”no cloud, no privacy concerns,
> completely offline. Our Palette Architect turns vibe prompts into dynamic gallery atmospheres in
> seconds. The Sequence Oracle analyzes your images and reorders them for maximum emotional impact,
> turning a random collection into a curated visual journey. And our AI Critic provides instant,
> constructive feedback on composition, emotion, and storytellingâ€”all without sending a single pixel
> to the cloud. This is what privacy-first creative AI looks like."

---

## For Next Agent: What You Need to Do

**CRITICAL FIRST STEP**: Fix the model inference issue. The model loads successfully but
`RunAnywhere.generateStream()` returns 0 tokens. THIS MUST BE FIXED FIRST.

**Read this document fully.** Then:

1. **FIX MODEL GENERATION**: Get ANYTHING to generate. Even "Hello World". This is mandatory.

2. **Once generation works**:
    - Test with mood preset prompts
    - Parse JSON responses
    - Apply to gallery

3. **Wire outputs**: Make sure mood presets actually change the gallery animation type, sequence
   oracle actually reorders images, and critique scores persist.

4. **Sync to backend**: POST AI results to Flask API, store in Supabase, display on web dashboard.

5. **Polish UI**: Animations, error states, loading indicatorsâ€”make it feel production-ready.

6. **Test end-to-end**: From dashboard â†’ editor â†’ AI tools â†’ apply â†’ view on web.

**Do NOT**:

- Create unnecessary documentation files (no AI_TESTING_GUIDE.md, no PHASE_1_SUMMARY.md, etc.)
- Modify files without understanding existing architecture
- Break existing functionality (test after every change)
- Make Git commits without user permission
- Waste time on over-engineeringâ€”ship features that work
- Give up on fixing the generation issue - this is the core of the hackathon

**Remember**: The goal is to **WIN the hackathon**. That means creative, polished, demo-ready AI
features that judges can see and understand in 5 minutes. Focus on impact over complexity. Make the
Palette Architect, Sequence Oracle, and AI Critic shine.

**The RunAnywhere SDK MUST work** - this is what the hackathon is about. Don't settle for mocking
it. Fix the generation issue first, then build on top of working inference.

Good luck! 
